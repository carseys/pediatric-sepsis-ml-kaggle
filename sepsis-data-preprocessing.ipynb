{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readin_data(data_type: str):\n",
    "    \"\"\" This function reads in test or train data, which must be in folders 'testing_data' and 'training_data' in the same directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_type' : str\n",
    "        This must be 'test' or 'train'.\n",
    "    \"\"\"\n",
    "    assert (data_type=='test') or (data_type=='train'), f'You gave data_type as {data_type}. Please define data_type as \"test\" or \"train.\"'\n",
    "    if data_type == 'test':\n",
    "        inner_directory = './testing_data/'\n",
    "        data_list = os.listdir('./testing_data')\n",
    "    else:\n",
    "        inner_directory = './training_data/'\n",
    "        data_list = os.listdir('./training_data')\n",
    "    data_dict = {}\n",
    "    for file_name in data_list:\n",
    "        data_dict[file_name.split('.')[0]] = pd.read_csv(inner_directory+file_name).drop_duplicates()\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('./processed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_uids(data_dictionary: dict):\n",
    "    \"\"\" This function adds a UID to each row to establish unique instances between person_id & measurement datetimes for the various tables.\n",
    "    \n",
    "    This is not done for the demographics file since the information in it is not sensitive to the hour.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "    \"\"\"\n",
    "    print(\"Adding UIDs.\")\n",
    "\n",
    "    for table_ind in list(data_dictionary.keys()):\n",
    "        if not table_ind.startswith(\"person_demographics\"):\n",
    "            table = data_dictionary[table_ind]\n",
    "            datetime_index = np.argmax([i.find('datetime') for i in table.columns])\n",
    "            date_column = table.columns[datetime_index]\n",
    "            personid_index = np.argmax([i.find('person_id') for i in table.columns])\n",
    "            personid_column = table.columns[personid_index]\n",
    "            table['uid'] = table[date_column].astype(str) + table[personid_column].astype(str)\n",
    "            table.drop(columns=[date_column,personid_column],inplace=True)\n",
    "            data_dictionary[table_ind] = table\n",
    "            # print(f'file {table_ind} with len {len(table)}')\n",
    "    \n",
    "    print(\"UIDs added\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def birthday_management(data_dictionary: dict):\n",
    "    \"\"\"\n",
    "    This function processes the 'person_demographics' table of given data, which is inputed as a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "    \"\"\"\n",
    "    demographics_ind_no = np.argmax([table.startswith(\"person_demographics\") for table in data_dictionary.keys()])\n",
    "    demographics_index = list(data_dictionary.keys())[demographics_ind_no]\n",
    "    demographics = data_dictionary[demographics_index]\n",
    "    print(f\"Beginning processing for {demographics_index}.\")\n",
    "    \n",
    "\n",
    "    new_birthday_col = pd.DataFrame(columns=['birthday_formatted', 'person_id'])\n",
    "    new_visit_col = pd.DataFrame(columns=['visit_start_date','new_visit_startdate'])\n",
    "    \n",
    "    for person in np.unique(demographics['person_id']):\n",
    "        birthday = demographics[demographics['person_id']==person]['birth_datetime'].to_list()[0]\n",
    "        birthday_formatted = datetime.strptime(birthday,'%Y-%m-%d')\n",
    "        new_birthday_col.loc[len(new_birthday_col)] = [birthday_formatted, person]\n",
    "\n",
    "    for date in np.unique(demographics['visit_start_date']):\n",
    "        visit_start = demographics[demographics['visit_start_date']==date]['visit_start_date'].to_list()[0]\n",
    "        new_visit_startdate = datetime.strptime(visit_start,'%Y-%m-%d')\n",
    "        # print(f'new {new_visit_startdate} old {date}')\n",
    "        new_visit_col.loc[len(new_visit_col)] = [date, new_visit_startdate]\n",
    "\n",
    "\n",
    "    demographics = pd.merge(left=demographics,right=new_birthday_col,how='left',on='person_id')\n",
    "    demographics = pd.merge(left=demographics,right=new_visit_col,how='left',on='visit_start_date')\n",
    "    demographics.drop(columns=['visit_start_date','birth_datetime'],inplace=True)\n",
    "    demographics.to_csv(f'./processed_data/processed_{demographics_index}.csv')\n",
    "    data_dictionary[demographics_index] = demographics\n",
    "    print(f\"Finished processing of {demographics_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurement_meds_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'measurement_meds' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "    \"\"\"\n",
    "    body_measurements_ind = np.argmax([table.startswith(\"measurement_meds\") for table in data_dictionary.keys()])\n",
    "    body_measurements_index = list(data_dictionary.keys())[body_measurements_ind]\n",
    "    measurements = data_dictionary[body_measurements_index]\n",
    "    print(f\"Beginning processing for {body_measurements_index}.\")\n",
    "\n",
    "    \n",
    "    measurements = measurements.dropna(subset=measurements.select_dtypes(float).columns, how='all')\n",
    "    # measurements.drop(index=[i for i in measurements[measurements['Body temperature']>45].index], axis=1,inplace=True)\n",
    "    measurements['Body temperature'] = measurements['Body temperature'].apply(lambda x: np.nan if x >46 else x)\n",
    "    measurements.to_csv(f'./processed_data/processed_{body_measurements_index}.csv')\n",
    "    data_dictionary[body_measurements_index] = measurements\n",
    "    print(f\"Finished processing of {body_measurements_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drugs_exposure_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'drugsexposure' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "\n",
    "    Requirements\n",
    "    ------------\n",
    "    You need to run add_uids first.\n",
    "    \"\"\"\n",
    "    # assert uids_added == True, 'You need to run add_uids before this function.'\n",
    "    drugs_exposure_ind = np.argmax([table.startswith(\"drugsexposure\") for table in data_dictionary.keys()])\n",
    "    drugs_exposure_index = list(data_dictionary.keys())[drugs_exposure_ind]\n",
    "    drugs_exposure = data_dictionary[drugs_exposure_index]\n",
    "    print(f\"Beginning processing for {drugs_exposure_index}.\")\n",
    "\n",
    "    drugs_exposure_processed = pd.DataFrame(columns = ['uid', 'drugs', 'routes', 'visit_occurrence_id'])\n",
    "    \n",
    "    for x in tqdm(np.unique(drugs_exposure['uid'])):\n",
    "        drugs = drugs_exposure[drugs_exposure['uid']==x]['drug_concept_id'].to_list()\n",
    "        drugs.sort()\n",
    "        try:\n",
    "            route = drugs_exposure[drugs_exposure['uid']==x]['route_concept_id'].to_list()\n",
    "            route = list(set(route))\n",
    "            route = [str(i) for i in route]\n",
    "            route.sort()\n",
    "        except:\n",
    "            route = drugs_exposure[drugs_exposure['uid']==x]['route_concept_id'].to_list()\n",
    "            route = list(set(route))\n",
    "        visit_occurrence = drugs_exposure[drugs_exposure['uid']==x]['visit_occurrence_id'].to_list()[0]\n",
    "        drugs_exposure_processed.loc[len(drugs_exposure_processed)]= [x,drugs,route, visit_occurrence]\n",
    "    data_dictionary[drugs_exposure_index] = drugs_exposure_processed\n",
    "    drugs_exposure.to_csv(f'./processed_data/processed_{drugs_exposure_index}.csv')\n",
    "    print(f\"Finished processing of {drugs_exposure_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurement_lab_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'measurement_lab' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "\n",
    "    Requirements\n",
    "    ------------\n",
    "    You need to run add_uids first.\n",
    "    \"\"\"\n",
    "    # assert uids_added == True, 'You need to run add_uids before this function.'\n",
    "\n",
    "    measurement_lab_ind = np.argmax([table.startswith(\"measurement_lab\") for table in data_dictionary.keys()])\n",
    "    measurement_lab_index = list(data_dictionary.keys())[measurement_lab_ind]\n",
    "    measurement_lab = data_dictionary[measurement_lab_index]\n",
    "    print(f\"Beginning processing for {measurement_lab_index}.\")\n",
    "\n",
    "    nan_col_inds = list(measurement_lab.isna().all())\n",
    "    nan_col_indices = list(measurement_lab.loc[:,nan_col_inds].columns)\n",
    "    measurement_lab.drop(columns=nan_col_indices, inplace=True)\n",
    "\n",
    "    measurement_lab = measurement_lab.dropna(subset=list(measurement_lab.select_dtypes(float).columns), how='all')\n",
    "    measurement_lab_count = pd.DataFrame([list(i) for i in Counter(measurement_lab['uid']).items()],columns=['uid','count'])\n",
    "    measurement_lab_count['count'].astype(int)\n",
    "    measurement_lab_rows = pd.DataFrame()\n",
    "    measurement_lab_extras = measurement_lab_count[measurement_lab_count['count']>1]\n",
    "    for j in [i for i in measurement_lab_extras['uid']]:\n",
    "        measurement_lab_rows = pd.concat([measurement_lab_rows,measurement_lab[measurement_lab['uid']==j].max().to_frame().T]).reset_index(drop=True)\n",
    "    measurement_lab = measurement_lab.drop(index=[i for i in measurement_lab[measurement_lab['uid']==measurement_lab_extras['uid'].item()].index], axis=1,inplace=False)\n",
    "    measurement_lab = pd.concat([measurement_lab,measurement_lab_rows]).reset_index(drop=True)\n",
    "\n",
    "    col_inds = [not((i.endswith('_id')) or (i=='uid')) for i in list(training_data['measurement_lab_train'].columns)]\n",
    "    col_names = measurement_lab.columns.values[col_inds]\n",
    "    for column in col_names:\n",
    "        measurement_lab[column] = measurement_lab[column].astype(float)\n",
    "\n",
    "    measurement_lab.to_csv(f'./processed_data/processed_{measurement_lab_index}.csv')\n",
    "    data_dictionary[measurement_lab_index] = measurement_lab\n",
    "    print(f\"Finished processing of {measurement_lab_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurement_observation_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'measurement_observation' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    measurement_obs_ind = np.argmax([table.startswith(\"measurement_observation\") for table in data_dictionary.keys()])\n",
    "    measurement_obs_index = list(data_dictionary.keys())[measurement_obs_ind]\n",
    "    measurement_obs = data_dictionary[measurement_obs_index]\n",
    "    print(f\"Beginning processing for {measurement_obs_index}.\")\n",
    "\n",
    "    # measurement_obs = measurement_obs.dropna(subset=measurement_obs.select_dtypes(float).columns, how='all')\n",
    "    measurement_obs.to_csv(f'./processed_data/processed_{measurement_obs_index}.csv')\n",
    "    data_dictionary[measurement_obs_index] = measurement_obs\n",
    "    print(f\"Finished processing of {measurement_obs_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'observation' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    observation_ind = np.argmax([table.startswith(\"observation\") for table in data_dictionary.keys()])\n",
    "    observation_index = list(data_dictionary.keys())[observation_ind]\n",
    "    observation = data_dictionary[observation_index]\n",
    "    print(f\"Beginning processing for {observation_index}.\")\n",
    "\n",
    "\n",
    "    observation = observation.dropna(subset=observation.select_dtypes(object).columns, how='all')\n",
    "\n",
    "    observation.to_csv(f'./processed_data/processed_{observation_index}.csv')\n",
    "\n",
    "    data_dictionary[observation_index] = observation \n",
    "    print(f\"Finished processing of {observation_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procedures_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'observation' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "\n",
    "    Requirements\n",
    "    ------------\n",
    "    You need to run add_uids first.\n",
    "    \"\"\"\n",
    "    # assert uids_added == True, 'You need to run add_uids before this function.'\n",
    "\n",
    "    procedures_ind = np.argmax([table.startswith(\"proceduresoccurrences\") for table in data_dictionary.keys()])\n",
    "    procedures_index = list(data_dictionary.keys())[procedures_ind]\n",
    "    procedures = data_dictionary[procedures_index]\n",
    "    print(f\"Beginning processing for {procedures_index}.\")\n",
    "\n",
    "    procedures = procedures.dropna(subset=procedures.select_dtypes(object).columns, how='all')\n",
    "\n",
    "\n",
    "    visit_id_index = np.argmax([i.find('visit_occurrence') for i in procedures.columns])\n",
    "    visit_column = procedures.columns[visit_id_index]\n",
    "    procedures.drop(columns=visit_column,inplace=True)\n",
    "    procedures.drop_duplicates(inplace=True)\n",
    "\n",
    "    procedures.to_csv(f'./processed_data/processed_{procedures_index}.csv')\n",
    "\n",
    "\n",
    "    data_dictionary[procedures_index] = procedures \n",
    "    print(f\"Finished processing of {procedures_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devices_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'devices' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    devices_ind = np.argmax([table.startswith(\"devices\") for table in data_dictionary.keys()])\n",
    "    devices_index = list(data_dictionary.keys())[devices_ind]\n",
    "    devices = data_dictionary[devices_index]\n",
    "    print(f\"Beginning processing for {devices_index}.\")\n",
    "\n",
    "    devices = devices.dropna(subset=devices.select_dtypes(object).columns, how='all')\n",
    "\n",
    "    devices.to_csv(f'./processed_data/processed_{devices_index}.csv')\n",
    "\n",
    "    data_dictionary[devices_index] = devices\n",
    "    print(f\"Finished processing of {devices_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepsis_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'sepsis' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "    \"\"\"\n",
    "    sepsis_ind = np.argmax([table.startswith(\"SepsisLabel\") for table in data_dictionary.keys()])\n",
    "    sepsis_index = list(data_dictionary.keys())[sepsis_ind]\n",
    "    sepsis = data_dictionary[sepsis_index]\n",
    "    print(f\"Beginning processing for {sepsis_index}.\")\n",
    "\n",
    "    #no NA values found:\n",
    "    sepsis = sepsis.dropna(subset=sepsis.select_dtypes(int).columns, how='all')\n",
    "    print(f\"Finished processing of {sepsis_index}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_type: str):\n",
    "    \"\"\" This function reads in test or train data and goes through functions to preprocess it. For further details see specific functions.\n",
    "\n",
    "    Processed tables will be saved into the /processed folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_type' : str\n",
    "        This must be 'test' or 'train'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    'processed_data' : dict\n",
    "        This is a dictionary of datatables that have been processed\n",
    "    'factors' : DataFrame\n",
    "        This is a DataFrame of the data from 'processed_data' joined together.\n",
    "    \"\"\"\n",
    "    assert (data_type=='test') or (data_type=='train'), f'You gave data_type as {data_type}. Please define data_type as \"test\" or \"train.\"'\n",
    "\n",
    "    if data_type == 'train':\n",
    "        training_data = readin_data('train')\n",
    "        add_uids(training_data)\n",
    "        birthday_management(training_data)\n",
    "        measurement_meds_processing(training_data)\n",
    "        drugs_exposure_processing(training_data)\n",
    "        measurement_lab_processing(training_data)\n",
    "        procedures_processing(training_data)\n",
    "\n",
    "        factors = pd.merge(left=training_data['measurement_meds_train'], right=training_data['measurement_lab_train'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=training_data['drugsexposure_train'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=training_data['proceduresoccurrences_train'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=training_data['person_demographics_episode_train'], how='outer',on='visit_occurrence_id')\n",
    "        factors = pd.merge(left=training_data['SepsisLabel_train'],right=factors,how='left',on='uid')\n",
    "        factors.to_csv(f'./processed_data/factors_train.csv')\n",
    "        processed_data = training_data\n",
    "\n",
    "    else:\n",
    "        testing_data = readin_data('test')\n",
    "        add_uids(testing_data)\n",
    "        birthday_management(testing_data)\n",
    "        measurement_meds_processing(testing_data)\n",
    "        drugs_exposure_processing(testing_data)\n",
    "        measurement_lab_processing(testing_data)\n",
    "\n",
    "        factors = pd.merge(left=testing_data['measurement_meds_test'], right=testing_data['measurement_lab_test'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=testing_data['drugsexposure_test'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=testing_data['person_demographics_episode_test'], how='outer',on='visit_occurrence_id')\n",
    "        factors = pd.merge(left=testing_data['SepsisLabel_test'],right=factors,how='left',on='uid')\n",
    "        factors.to_csv(f'./processed_data/factors_test.csv')\n",
    "        processed_data = testing_data\n",
    "\n",
    "    factors.drop(columns=['visit_occurrence_id_x','visit_occurrence_id_y'],inplace=True)\n",
    "\n",
    "    return processed_data, factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding UIDs.\n",
      "UIDs added\n",
      "Beginning processing for person_demographics_episode_train.\n",
      "Finished processing of person_demographics_episode_train\n",
      "Beginning processing for measurement_meds_train.\n",
      "Finished processing of measurement_meds_train\n",
      "Beginning processing for drugsexposure_train.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150407/150407 [2:00:55<00:00, 20.73it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing of drugsexposure_train\n",
      "Beginning processing for measurement_lab_train.\n",
      "Finished processing of measurement_lab_train\n",
      "Beginning processing for proceduresoccurrences_train.\n",
      "Finished processing of proceduresoccurrences_train\n"
     ]
    }
   ],
   "source": [
    "training_data, data_factors = process_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(data_directory: dict, data_type: str):\n",
    "    \"\"\" This function takes a data dictionary of tables and merges to have a table of factors for categorical encoding etc.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_directory' : dict\n",
    "        This is a data dictionary of pandas DataFrames.\n",
    "    'data_type' : str\n",
    "        This must be 'test' or 'train'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    'factors' : DataFrame\n",
    "        This is a DataFrame of the data from 'processed_data' joined together.    \n",
    "    \"\"\"\n",
    "    assert (data_type=='test') or (data_type=='train'), f'You gave data_type as {data_type}. Please define data_type as \"test\" or \"train.\"'\n",
    "\n",
    "    if data_type == 'train':\n",
    "        training_data = data_directory.copy()\n",
    "\n",
    "        factors = pd.merge(left=training_data['measurement_meds_train'], right=training_data['measurement_lab_train'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=training_data['drugsexposure_train'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=training_data['proceduresoccurrences_train'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=training_data['person_demographics_episode_train'], how='outer',on='visit_occurrence_id')\n",
    "        factors = pd.merge(left=training_data['SepsisLabel_train'],right=factors,how='left',on='uid')\n",
    "\n",
    "    else:\n",
    "        testing_data = data_directory.copy()\n",
    "\n",
    "        factors = pd.merge(left=testing_data['measurement_meds_test'], right=testing_data['measurement_lab_test'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=testing_data['drugsexposure_test'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=testing_data['person_demographics_episode_test'], how='outer',on='visit_occurrence_id')\n",
    "        factors = pd.merge(left=testing_data['SepsisLabel_test'],right=factors,how='left',on='uid')\n",
    "\n",
    "    factors.to_csv(f'./processed_data/processed_{factors}.csv')\n",
    "\n",
    "    return factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populating_birthdays(df):\n",
    "    big_birthday_col = pd.DataFrame()\n",
    "    for person in np.unique(df['person_id']):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factors_2 = data_factors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_2 = training_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_meas_lab(data_dictionary: dict):\n",
    "    measurement_lab_ind = np.argmax([table.startswith(\"measurement_lab\") for table in data_dictionary.keys()])\n",
    "    measurement_lab_index = list(data_dictionary.keys())[measurement_lab_ind]\n",
    "    measurement_lab = data_dictionary[measurement_lab_index]\n",
    "    col_inds = [not((i.endswith('_id')) or (i=='uid')) for i in list(training_data['measurement_lab_train'].columns)]\n",
    "    col_names = measurement_lab.columns.values[col_inds]\n",
    "    for column in col_names:\n",
    "        measurement_lab[column] = measurement_lab[column].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently Base excess in Venous blood by calculation\n",
      "currently Base excess in Arterial blood by calculation\n",
      "currently Phosphate [Moles/volume] in Serum or Plasma\n",
      "currently Potassium [Moles/volume] in Blood\n",
      "currently Bilirubin.total [Moles/volume] in Serum or Plasma\n",
      "currently Neutrophil Ab [Units/volume] in Serum\n",
      "currently Bicarbonate [Moles/volume] in Arterial blood\n",
      "currently Hematocrit [Volume Fraction] of Blood\n",
      "currently Glucose [Moles/volume] in Serum or Plasma\n",
      "currently Calcium [Moles/volume] in Serum or Plasma\n",
      "currently Chloride [Moles/volume] in Blood\n",
      "currently Sodium [Moles/volume] in Serum or Plasma\n",
      "currently C reactive protein [Mass/volume] in Serum or Plasma\n",
      "currently Carbon dioxide [Partial pressure] in Venous blood\n",
      "currently Oxygen [Partial pressure] in Venous blood\n",
      "currently Albumin [Mass/volume] in Serum or Plasma\n",
      "currently Bicarbonate [Moles/volume] in Venous blood\n",
      "currently Oxygen [Partial pressure] in Arterial blood\n",
      "currently Carbon dioxide [Partial pressure] in Arterial blood\n",
      "currently Interleukin 6 [Mass/volume] in Body fluid\n",
      "currently Magnesium [Moles/volume] in Blood\n",
      "currently Prothrombin time (PT)\n",
      "currently Procalcitonin [Mass/volume] in Serum or Plasma\n",
      "currently Lactate [Moles/volume] in Blood\n",
      "currently Creatinine [Mass/volume] in Blood\n",
      "currently Fibrinogen measurement\n",
      "currently Bilirubin measurement\n",
      "currently Partial thromboplastin time\n",
      "currently  activated\n",
      "currently Total white blood count\n",
      "currently Platelet count\n",
      "currently White blood cell count\n",
      "currently Blood venous pH\n",
      "currently D-dimer level\n",
      "currently Blood arterial pH\n",
      "currently Hemoglobin [Moles/volume] in Blood\n"
     ]
    }
   ],
   "source": [
    "fix_meas_lab(training_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_factors = merge_data(training_data_2, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_encoding(df):\n",
    "    categorical_cols = list(df.select_dtypes(object).columns)\n",
    "    le_dictionary = {}\n",
    "    for name in tqdm(categorical_cols):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(df.loc[:,f'{name}'])\n",
    "        new_col = le.transform(df.loc[:,f'{name}'])\n",
    "        le_dictionary[name] = le\n",
    "        df.drop(columns=f'{name}', inplace=True)\n",
    "        df[f'new_{name}'] = new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_modeling(df):\n",
    "    column_list = []\n",
    "    [column_list.append(i) for i in range(3,46)]\n",
    "    X = df.iloc[:,column_list].values\n",
    "    y = df.iloc[:,1].values\n",
    "    person_id = df.iloc[:,0].values\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2)\n",
    "    for train_x_index, test_x_index in gss.split(X=X,y=y,groups=person_id):\n",
    "        X_train = X[train_x_index]\n",
    "        X_test = X[test_x_index]\n",
    "        y_train = y[train_x_index]\n",
    "        y_test = y[test_x_index]\n",
    "        person_id_train = person_id[train_x_index]\n",
    "        person_id_test = person_id[test_x_index]\n",
    "        \n",
    "    formatted_data = {}\n",
    "    formatted_data['X_train'] = X_train\n",
    "    formatted_data['X_test'] = X_test\n",
    "    formatted_data['y_train'] = y_train\n",
    "    formatted_data['y_test'] = y_test\n",
    "    formatted_data['person_id_train'] = person_id_train\n",
    "    formatted_data['person_id_test'] = person_id_test\n",
    "\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_model(data_dictionary: dict):\n",
    "    X_train = data_dictionary['X_train']\n",
    "    X_test = data_dictionary['X_test']\n",
    "    y_train = data_dictionary['y_train']\n",
    "    y_test = data_dictionary['y_test']\n",
    "    person_id_train = data_dictionary['person_id_train']\n",
    "    person_id_test = data_dictionary['person_id_test']\n",
    "\n",
    "    r_forest_model = RandomForestClassifier()\n",
    "    r_forest_model.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = forest_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy score: {accuracy}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=['not sepsis', 'sepsis'], yticklabels=['not sepsis', 'sepsis'])\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show(block=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

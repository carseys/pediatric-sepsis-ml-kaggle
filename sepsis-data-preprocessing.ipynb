{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay, f1_score\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import KNNImputer\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readin_data(data_type: str):\n",
    "    \"\"\" This function reads in test or train data, which must be in folders 'testing_data' and 'training_data' in the same directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_type' : str\n",
    "        This must be 'test' or 'train'.\n",
    "    \"\"\"\n",
    "    assert (data_type=='test') or (data_type=='train'), f'You gave data_type as {data_type}. Please define data_type as \"test\" or \"train.\"'\n",
    "    if data_type == 'test':\n",
    "        inner_directory = './testing_data/'\n",
    "        data_list = os.listdir('./testing_data')\n",
    "    else:\n",
    "        inner_directory = './training_data/'\n",
    "        data_list = os.listdir('./training_data')\n",
    "    data_dict = {}\n",
    "    for file_name in data_list:\n",
    "        data_dict[file_name.split('.')[0]] = pd.read_csv(inner_directory+file_name).drop_duplicates()\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_data_directory():\n",
    "    \"\"\" Makes 'processed_data' directory if one is not found.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Add flag for this function\n",
    "    os.makedirs('./processed_data', exist_ok=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_uids(data_dictionary: dict):\n",
    "    \"\"\" This function adds a UID to each row to establish unique instances between person_id & measurement datetimes for the various tables.\n",
    "    \n",
    "    This is not done for the demographics file since the information in it is not sensitive to the hour.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "    \n",
    "    Details\n",
    "    -------\n",
    "    * UID is a concatenation of datetime and person_id, in that order.\n",
    "    * UID is later used as a key for table joins.\n",
    "    \"\"\"\n",
    "    print(\"Adding UIDs.\")\n",
    "\n",
    "    for table_ind in list(data_dictionary.keys()):\n",
    "        if not table_ind.startswith(\"person_demographics\"):\n",
    "            table = data_dictionary[table_ind]\n",
    "            datetime_index = np.argmax([i.find('datetime') for i in table.columns])\n",
    "            date_column = table.columns[datetime_index]\n",
    "            personid_index = np.argmax([i.find('person_id') for i in table.columns])\n",
    "            personid_column = table.columns[personid_index]\n",
    "            table['uid'] = table[date_column].astype(str) + table[personid_column].astype(str)\n",
    "            table.drop(columns=[date_column,personid_column],inplace=True)\n",
    "            data_dictionary[table_ind] = table\n",
    "            # print(f'file {table_ind} with len {len(table)}')\n",
    "    \n",
    "    print(\"UIDs added\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def birthday_management(data_dictionary: dict):\n",
    "    \"\"\"\n",
    "    This function processes the 'person_demographics' table of given data, which is inputed as a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "    \n",
    "    Details\n",
    "    -------\n",
    "    * adds new birthday and visit start date columns with dates formated using datetime package.\n",
    "    * joins new columns to old table using left join to match the unprocessed to the processed date\n",
    "    \"\"\"\n",
    "    demographics_ind_no = np.argmax([table.startswith(\"person_demographics\") for table in data_dictionary.keys()])\n",
    "    demographics_index = list(data_dictionary.keys())[demographics_ind_no]\n",
    "    demographics = data_dictionary[demographics_index]\n",
    "    print(f\"Beginning processing for {demographics_index}.\")\n",
    "    \n",
    "\n",
    "    new_birthday_col = pd.DataFrame(columns=['birthday_formatted', 'person_id'])\n",
    "    new_visit_col = pd.DataFrame(columns=['visit_start_date','new_visit_startdate'])\n",
    "    \n",
    "    for person in np.unique(demographics['person_id']):\n",
    "        birthday = demographics[demographics['person_id']==person]['birth_datetime'].to_list()[0]\n",
    "        birthday_formatted = datetime.strptime(birthday,'%Y-%m-%d')\n",
    "        new_birthday_col.loc[len(new_birthday_col)] = [birthday_formatted, person]\n",
    "\n",
    "    for date in np.unique(demographics['visit_start_date']):\n",
    "        visit_start = demographics[demographics['visit_start_date']==date]['visit_start_date'].to_list()[0]\n",
    "        new_visit_startdate = datetime.strptime(visit_start,'%Y-%m-%d')\n",
    "        # print(f'new {new_visit_startdate} old {date}')\n",
    "        new_visit_col.loc[len(new_visit_col)] = [date, new_visit_startdate]\n",
    "\n",
    "\n",
    "    demographics = pd.merge(left=demographics,right=new_birthday_col,how='left',on='person_id')\n",
    "    demographics = pd.merge(left=demographics,right=new_visit_col,how='left',on='visit_start_date')\n",
    "    demographics.drop(columns=['visit_start_date','birth_datetime'],inplace=True)\n",
    "    demographics.to_csv(f'./processed_data/processed_{demographics_index}.csv')\n",
    "    data_dictionary[demographics_index] = demographics\n",
    "    print(f\"Finished processing of {demographics_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurement_meds_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'measurement_meds' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "\n",
    "    Details\n",
    "    -------\n",
    "    * removes body temperature measurements > 46 C\n",
    "    \"\"\"\n",
    "    body_measurements_ind = np.argmax([table.startswith(\"measurement_meds\") for table in data_dictionary.keys()])\n",
    "    body_measurements_index = list(data_dictionary.keys())[body_measurements_ind]\n",
    "    measurements = data_dictionary[body_measurements_index]\n",
    "    print(f\"Beginning processing for {body_measurements_index}.\")\n",
    "\n",
    "    \n",
    "    measurements = measurements.dropna(subset=measurements.select_dtypes(float).columns, how='all')\n",
    "    # measurements.drop(index=[i for i in measurements[measurements['Body temperature']>45].index], axis=1,inplace=True)\n",
    "    measurements['Body temperature'] = measurements['Body temperature'].apply(lambda x: np.nan if x > 46 else x)\n",
    "    measurements.to_csv(f'./processed_data/processed_{body_measurements_index}.csv')\n",
    "    data_dictionary[body_measurements_index] = measurements\n",
    "    print(f\"Finished processing of {body_measurements_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drugs_exposure_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'drugsexposure' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "\n",
    "    Requirements\n",
    "    ------------\n",
    "    You need to run add_uids first.\n",
    "\n",
    "    Details\n",
    "    -------\n",
    "    * combines rows of the same datetime with different drugs to be one row per datetime with all drugs listed in new 'drugs' column and all drug routes listed in new 'routes' column\n",
    "    * converts from list to string for new columns to allow categorical encoding later on\n",
    "    \"\"\"\n",
    "    # assert uids_added == True, 'You need to run add_uids before this function.'\n",
    "    drugs_exposure_ind = np.argmax([table.startswith(\"drugsexposure\") for table in data_dictionary.keys()])\n",
    "    drugs_exposure_index = list(data_dictionary.keys())[drugs_exposure_ind]\n",
    "    drugs_exposure = data_dictionary[drugs_exposure_index]\n",
    "    print(f\"Beginning processing for {drugs_exposure_index}.\")\n",
    "\n",
    "    drugs_exposure_processed = pd.DataFrame(columns = ['uid', 'drugs', 'routes', 'visit_occurrence_id'])\n",
    "    \n",
    "    for x in tqdm(np.unique(drugs_exposure['uid'])):\n",
    "        drugs = drugs_exposure[drugs_exposure['uid']==x]['drug_concept_id'].to_list()\n",
    "        drugs.sort()\n",
    "        try:\n",
    "            route = drugs_exposure[drugs_exposure['uid']==x]['route_concept_id'].to_list()\n",
    "            route = list(set(route))\n",
    "            route = [str(i) for i in route]\n",
    "            route.sort()\n",
    "        except:\n",
    "            route = drugs_exposure[drugs_exposure['uid']==x]['route_concept_id'].to_list()\n",
    "            route = list(set(route))\n",
    "        visit_occurrence = drugs_exposure[drugs_exposure['uid']==x]['visit_occurrence_id'].to_list()[0]\n",
    "        drugs_exposure_processed.loc[len(drugs_exposure_processed)]= [x,drugs,route, visit_occurrence]\n",
    "    # switching format from list to string for later processing of categorical data:\n",
    "    for row in drugs_exposure_processed['drugs']:\n",
    "        row = str(row)[1:-1]\n",
    "    for row in drugs_exposure_processed['routes']:\n",
    "        row = str(row)[1:-1]\n",
    "\n",
    "    drugs_exposure_processed['drugs'] = drugs_exposure_processed['drugs'].apply(lambda x: str(x)[1:-1])\n",
    "    drugs_exposure_processed['drugs'] = drugs_exposure_processed['drugs'].apply(lambda x: np.nan if x=='a' else x)\n",
    "\n",
    "\n",
    "    drugs_exposure_processed['routes'] = drugs_exposure_processed['routes'].apply(lambda x: np.nan if x is str else str(x)[1:-1])\n",
    "    drugs_exposure_processed['routes'] = drugs_exposure_processed['routes'].apply(lambda x: np.nan if x=='a' else x)\n",
    "\n",
    "    data_dictionary[drugs_exposure_index] = drugs_exposure_processed\n",
    "\n",
    "\n",
    "    drugs_exposure.to_csv(f'./processed_data/processed_{drugs_exposure_index}.csv')\n",
    "    print(f\"Finished processing of {drugs_exposure_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurement_lab_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'measurement_lab' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "\n",
    "    Requirements\n",
    "    ------------\n",
    "    You need to run add_uids first.\n",
    "\n",
    "    Details\n",
    "    -------\n",
    "    * removes rows which are all NA\n",
    "    * combines rows which have the same datetime but different columns filled (different columns have non-NA values)\n",
    "    * converts columns to float to resolve typing issue\n",
    "    \"\"\"\n",
    "    # assert uids_added == True, 'You need to run add_uids before this function.'\n",
    "\n",
    "    measurement_lab_ind = np.argmax([table.startswith(\"measurement_lab\") for table in data_dictionary.keys()])\n",
    "    measurement_lab_index = list(data_dictionary.keys())[measurement_lab_ind]\n",
    "    measurement_lab = data_dictionary[measurement_lab_index]\n",
    "    print(f\"Beginning processing for {measurement_lab_index}.\")\n",
    "\n",
    "    nan_col_inds = list(measurement_lab.isna().all())\n",
    "    nan_col_indices = list(measurement_lab.loc[:,nan_col_inds].columns)\n",
    "    measurement_lab.drop(columns=nan_col_indices, inplace=True)\n",
    "\n",
    "    measurement_lab = measurement_lab.dropna(subset=list(measurement_lab.select_dtypes(float).columns), how='all')\n",
    "    measurement_lab_count = pd.DataFrame([list(i) for i in Counter(measurement_lab['uid']).items()],columns=['uid','count'])\n",
    "    measurement_lab_count['count'].astype(int)\n",
    "    measurement_lab_rows = pd.DataFrame()\n",
    "    measurement_lab_extras = measurement_lab_count[measurement_lab_count['count']>1]\n",
    "    for j in [i for i in measurement_lab_extras['uid']]:\n",
    "        measurement_lab_rows = pd.concat([measurement_lab_rows,measurement_lab[measurement_lab['uid']==j].max().to_frame().T]).reset_index(drop=True)\n",
    "    measurement_lab = measurement_lab.drop(index=[i for i in measurement_lab[measurement_lab['uid']==measurement_lab_extras['uid'].item()].index], axis=1,inplace=False)\n",
    "    measurement_lab = pd.concat([measurement_lab,measurement_lab_rows]).reset_index(drop=True)\n",
    "\n",
    "    col_inds = [not((i.endswith('_id')) or (i=='uid')) for i in list(measurement_lab.columns)]\n",
    "    col_names = measurement_lab.columns.values[col_inds]\n",
    "    for column in col_names:\n",
    "        measurement_lab[column] = measurement_lab[column].astype(float)\n",
    "\n",
    "    measurement_lab.to_csv(f'./processed_data/processed_{measurement_lab_index}.csv')\n",
    "    data_dictionary[measurement_lab_index] = measurement_lab\n",
    "    print(f\"Finished processing of {measurement_lab_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurement_observation_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'measurement_observation' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "    \n",
    "    Details\n",
    "    -------\n",
    "    * creates csv to verify this table has been processed.\n",
    "    \"\"\"\n",
    "\n",
    "    measurement_obs_ind = np.argmax([table.startswith(\"measurement_observation\") for table in data_dictionary.keys()])\n",
    "    measurement_obs_index = list(data_dictionary.keys())[measurement_obs_ind]\n",
    "    measurement_obs = data_dictionary[measurement_obs_index]\n",
    "    print(f\"Beginning processing for {measurement_obs_index}.\")\n",
    "\n",
    "    # measurement_obs = measurement_obs.dropna(subset=measurement_obs.select_dtypes(float).columns, how='all')\n",
    "    measurement_obs.to_csv(f'./processed_data/processed_{measurement_obs_index}.csv')\n",
    "    data_dictionary[measurement_obs_index] = measurement_obs\n",
    "    print(f\"Finished processing of {measurement_obs_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'observation' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "    \n",
    "    Details\n",
    "    -------\n",
    "    * creates csv to verify this table has been processed.\n",
    "    \"\"\"\n",
    "\n",
    "    observation_ind = np.argmax([table.startswith(\"observation\") for table in data_dictionary.keys()])\n",
    "    observation_index = list(data_dictionary.keys())[observation_ind]\n",
    "    observation = data_dictionary[observation_index]\n",
    "    print(f\"Beginning processing for {observation_index}.\")\n",
    "\n",
    "\n",
    "    observation = observation.dropna(subset=observation.select_dtypes(object).columns, how='all')\n",
    "\n",
    "    observation.to_csv(f'./processed_data/processed_{observation_index}.csv')\n",
    "\n",
    "    data_dictionary[observation_index] = observation \n",
    "    print(f\"Finished processing of {observation_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procedures_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'observation' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "\n",
    "    Requirements\n",
    "    ------------\n",
    "    You need to run add_uids first.\n",
    "    \n",
    "    Details\n",
    "    -------\n",
    "    * drops visit_occurrence column\n",
    "    * drops duplicate entries\n",
    "    \"\"\"\n",
    "    # assert uids_added == True, 'You need to run add_uids before this function.'\n",
    "\n",
    "    procedures_ind = np.argmax([table.startswith(\"proceduresoccurrences\") for table in data_dictionary.keys()])\n",
    "    procedures_index = list(data_dictionary.keys())[procedures_ind]\n",
    "    procedures = data_dictionary[procedures_index]\n",
    "    print(f\"Beginning processing for {procedures_index}.\")\n",
    "\n",
    "    procedures = procedures.dropna(subset=procedures.select_dtypes(object).columns, how='all')\n",
    "\n",
    "\n",
    "    visit_id_index = np.argmax([i.find('visit_occurrence') for i in procedures.columns])\n",
    "    visit_column = procedures.columns[visit_id_index]\n",
    "    procedures.drop(columns=visit_column,inplace=True)\n",
    "    procedures.drop_duplicates(inplace=True)\n",
    "\n",
    "    procedures.to_csv(f'./processed_data/processed_{procedures_index}.csv')\n",
    "\n",
    "\n",
    "    data_dictionary[procedures_index] = procedures \n",
    "    print(f\"Finished processing of {procedures_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devices_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'devices' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "    \n",
    "    Details\n",
    "    -------\n",
    "    * creates csv to verify this table has been processed.\n",
    "    \"\"\"\n",
    "\n",
    "    devices_ind = np.argmax([table.startswith(\"devices\") for table in data_dictionary.keys()])\n",
    "    devices_index = list(data_dictionary.keys())[devices_ind]\n",
    "    devices = data_dictionary[devices_index]\n",
    "    print(f\"Beginning processing for {devices_index}.\")\n",
    "\n",
    "    devices = devices.dropna(subset=devices.select_dtypes(object).columns, how='all')\n",
    "\n",
    "    devices.to_csv(f'./processed_data/processed_{devices_index}.csv')\n",
    "\n",
    "    data_dictionary[devices_index] = devices\n",
    "    print(f\"Finished processing of {devices_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepsis_processing(data_dictionary: dict):\n",
    "    \"\"\" This function processes the 'sepsis' table of given data, which is inputed in a dictionary. The data in dictionary is replaced by index, hence function returns nothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_dictionary' : dict\n",
    "        A dictionary of pandas DataFrames.\n",
    "    \n",
    "    Details\n",
    "    -------\n",
    "    * drops values which have no datetime\n",
    "    \"\"\"\n",
    "    sepsis_ind = np.argmax([table.startswith(\"SepsisLabel\") for table in data_dictionary.keys()])\n",
    "    sepsis_index = list(data_dictionary.keys())[sepsis_ind]\n",
    "    sepsis = data_dictionary[sepsis_index]\n",
    "    print(f\"Beginning processing for {sepsis_index}.\")\n",
    "\n",
    "    #no NA values found:\n",
    "    sepsis = sepsis.dropna(subset=sepsis.select_dtypes(int).columns, how='all')\n",
    "    #Taking out values that have no datetime:\n",
    "    no_time_rows = list(sepsis.loc[sepsis['uid'].str.startswith('nan', na=False)].index)\n",
    "    print(no_time_rows)\n",
    "    sepsis = sepsis.drop(index=no_time_rows, axis = 1, inplace = False)\n",
    "    \n",
    "    data_dictionary[sepsis_index] = sepsis\n",
    "    sepsis.to_csv(f'./processed_data/processed_{sepsis_index}.csv')\n",
    "    \n",
    "    print(f\"Finished processing of {sepsis_index}.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_type: str, load_tables: str):\n",
    "    \"\"\" This function reads in test or train data and goes through functions to preprocess it. For further details see specific functions.\n",
    "\n",
    "    Processed tables will be saved into the /processed folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_type' : str\n",
    "        This must be 'test' or 'train'. Determines which data to process.\n",
    "    'load_tables' : str\n",
    "        This must be 'yes' or 'no'. 'yes' means load csvs from processed_data folder, of the type given in 'data_type' input. 'no' means process data from training or testing folder, depending on 'data_type' input given.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    'processed_data' : dict\n",
    "        This is a dictionary of datatables that have been processed\n",
    "    'factors' : DataFrame\n",
    "        This is a DataFrame of the data from 'processed_data' joined together.\n",
    "    \"\"\"\n",
    "    assert (data_type=='test') or (data_type=='train'), f'You gave data_type as {data_type}. Please define data_type as \"test\" or \"train.\"'\n",
    "    assert (load_tables=='yes') or (load_tables=='no'), f'You gave load_tables as {load_tables}. Please define load_tables as \"test\" or \"train.\"'\n",
    "\n",
    "    processed_data_directory()\n",
    "\n",
    "    if data_type == 'train':\n",
    "        if load_tables == 'no':\n",
    "            training_data = readin_data('train')\n",
    "            add_uids(training_data)\n",
    "            birthday_management(training_data)\n",
    "            measurement_meds_processing(training_data)\n",
    "            drugs_exposure_processing(training_data)\n",
    "            measurement_lab_processing(training_data)\n",
    "            procedures_processing(training_data)\n",
    "            observation_processing(training_data)\n",
    "            measurement_observation_processing(training_data)\n",
    "            devices_processing(training_data)\n",
    "            sepsis_processing(training_data)\n",
    "        else:\n",
    "            training_data={}\n",
    "            inner_directory = './processed_data/'\n",
    "            data_list = os.listdir('./processed_data')\n",
    "            separator = '_'\n",
    "            for file_name in data_list:\n",
    "                if file_name.split('.')[0].split('_')[-1]=='train':\n",
    "                    training_data[separator.join(file_name.split('.')[0].split('_')[1:])] = pd.read_csv(inner_directory+file_name,index_col=0).drop_duplicates()\n",
    "\n",
    "        factors = pd.merge(left=training_data['measurement_meds_train'], right=training_data['measurement_lab_train'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=training_data['drugsexposure_train'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=training_data['proceduresoccurrences_train'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=training_data['devices_train'],how='outer',on='uid')\n",
    "        # factors = pd.merge(left=factors, right=training_data['person_demographics_episode_train'], how='outer',on='visit_occurrence_id')\n",
    "        factors = pd.merge(left=training_data['SepsisLabel_train'],right=factors,how='left',on='uid')\n",
    "        factors.to_csv(f'./processed_data/factors_train.csv')\n",
    "        processed_data = training_data\n",
    "\n",
    "    else:\n",
    "        if load_tables == 'no':\n",
    "            testing_data = readin_data('test')\n",
    "            add_uids(testing_data)\n",
    "            birthday_management(testing_data)\n",
    "            measurement_meds_processing(testing_data)\n",
    "            drugs_exposure_processing(testing_data)\n",
    "            measurement_lab_processing(testing_data)\n",
    "            procedures_processing(testing_data)\n",
    "            observation_processing(testing_data)\n",
    "            measurement_observation_processing(testing_data)\n",
    "            devices_processing(testing_data)\n",
    "            sepsis_processing(testing_data)\n",
    "        else:\n",
    "            training_data={}\n",
    "            inner_directory = './processed_data/'\n",
    "            data_list = os.listdir('./processed_data')\n",
    "            separator = '_'\n",
    "            for file_name in data_list:\n",
    "                if file_name.split('.')[0].split('_')[-1]=='test':\n",
    "                    training_data[separator.join(file_name.split('.')[0].split('_')[1:])] = pd.read_csv(inner_directory+file_name,index_col=0).drop_duplicates()\n",
    "            \n",
    "        factors = pd.merge(left=testing_data['measurement_meds_test'], right=testing_data['measurement_lab_test'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=testing_data['drugsexposure_test'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=training_data['devices_train'],how='outer',on='uid')\n",
    "        # factors = pd.merge(left=factors, right=testing_data['person_demographics_episode_test'], how='outer',on='visit_occurrence_id')\n",
    "        factors = pd.merge(left=testing_data['SepsisLabel_test'],right=factors,how='left',on='uid')\n",
    "        factors.to_csv(f'./processed_data/factors_test.csv')\n",
    "        processed_data = testing_data\n",
    "\n",
    "    factors.drop(columns=['visit_occurrence_id_x','visit_occurrence_id_y'],inplace=True)\n",
    "\n",
    "    return processed_data, factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_data, trained_factors = process_data(data_type='train', load_tables='yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding UIDs.\n",
      "UIDs added\n",
      "Beginning processing for person_demographics_episode_train.\n",
      "Finished processing of person_demographics_episode_train.\n",
      "Beginning processing for measurement_meds_train.\n",
      "Finished processing of measurement_meds_train.\n",
      "Beginning processing for drugsexposure_train.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150407/150407 [1:42:24<00:00, 24.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing of drugsexposure_train.\n",
      "Beginning processing for measurement_lab_train.\n",
      "Finished processing of measurement_lab_train.\n",
      "Beginning processing for proceduresoccurrences_train.\n",
      "Finished processing of proceduresoccurrences_train.\n",
      "Beginning processing for observation_train.\n",
      "Finished processing of observation_train.\n",
      "Beginning processing for measurement_observation_train.\n",
      "Finished processing of measurement_observation_train.\n",
      "Beginning processing for devices_train.\n",
      "Finished processing of devices_train.\n",
      "Beginning processing for SepsisLabel_train.\n",
      "[9334, 31978, 49555, 101232, 103612, 132662, 136028, 137412, 138129, 176938, 191287, 236491, 241192, 242348, 250585]\n",
      "Finished processing of SepsisLabel_train.\n"
     ]
    }
   ],
   "source": [
    "training_data, data_factors = process_data(data_type='train', load_tables='no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(data_directory: dict, data_type: str):\n",
    "    \"\"\" This function takes a data dictionary of tables and merges to have a table of factors for categorical encoding etc.\n",
    "\n",
    "    \n",
    "    note that this function is currently untested.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    'data_directory' : dict\n",
    "        This is a data dictionary of pandas DataFrames.\n",
    "    'data_type' : str\n",
    "        This must be 'test' or 'train'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    'factors' : DataFrame\n",
    "        This is a DataFrame of the data from 'processed_data' joined together.    \n",
    "    \"\"\"\n",
    "    assert (data_type=='test') or (data_type=='train'), f'You gave data_type as {data_type}. Please define data_type as \"test\" or \"train.\"'\n",
    "\n",
    "    if data_type == 'train':\n",
    "        training_data = data_directory.copy()\n",
    "\n",
    "        factors = pd.merge(left=training_data['measurement_meds_train'], right=training_data['measurement_lab_train'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=training_data['drugsexposure_train'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=training_data['proceduresoccurrences_train'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=training_data['person_demographics_episode_train'], how='outer',on='visit_occurrence_id')\n",
    "        factors = pd.merge(left=training_data['SepsisLabel_train'],right=factors,how='left',on='uid')\n",
    "\n",
    "    else:\n",
    "        testing_data = data_directory.copy()\n",
    "\n",
    "        factors = pd.merge(left=testing_data['measurement_meds_test'], right=testing_data['measurement_lab_test'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=testing_data['drugsexposure_test'],how='outer',on='uid')\n",
    "        factors = pd.merge(left=factors, right=testing_data['person_demographics_episode_test'], how='outer',on='visit_occurrence_id')\n",
    "        factors = pd.merge(left=testing_data['SepsisLabel_test'],right=factors,how='left',on='uid')\n",
    "\n",
    "    factors.to_csv(f'./processed_data/processed_{factors}.csv')\n",
    "\n",
    "    return factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factors_2 = data_factors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 331627 entries, 0 to 331626\n",
      "Data columns (total 49 columns):\n",
      " #   Column                                               Non-Null Count   Dtype  \n",
      "---  ------                                               --------------   -----  \n",
      " 0   SepsisLabel                                          331627 non-null  int64  \n",
      " 1   uid                                                  331627 non-null  object \n",
      " 2   Systolic blood pressure                              38380 non-null   float64\n",
      " 3   Diastolic blood pressure                             38367 non-null   float64\n",
      " 4   Body temperature                                     198709 non-null  float64\n",
      " 5   Respiratory rate                                     79352 non-null   float64\n",
      " 6   Heart rate                                           92769 non-null   float64\n",
      " 7   Measurement of oxygen saturation at periphery        92613 non-null   float64\n",
      " 8   Oxygen/Gas total [Pure volume fraction] Inhaled gas  2043 non-null    float64\n",
      " 9   Base excess in Venous blood by calculation           19054 non-null   float64\n",
      " 10  Base excess in Arterial blood by calculation         2413 non-null    float64\n",
      " 11  Phosphate [Moles/volume] in Serum or Plasma          10056 non-null   float64\n",
      " 12  Potassium [Moles/volume] in Blood                    31660 non-null   float64\n",
      " 13  Bilirubin.total [Moles/volume] in Serum or Plasma    14595 non-null   float64\n",
      " 14  Neutrophil Ab [Units/volume] in Serum                24638 non-null   float64\n",
      " 15  Bicarbonate [Moles/volume] in Arterial blood         2413 non-null    float64\n",
      " 16  Hematocrit [Volume Fraction] of Blood                27697 non-null   float64\n",
      " 17  Glucose [Moles/volume] in Serum or Plasma            16326 non-null   float64\n",
      " 18  Calcium [Moles/volume] in Serum or Plasma            8349 non-null    float64\n",
      " 19  Chloride [Moles/volume] in Blood                     32508 non-null   float64\n",
      " 20  Sodium [Moles/volume] in Serum or Plasma             32456 non-null   float64\n",
      " 21  C reactive protein [Mass/volume] in Serum or Plasma  22914 non-null   float64\n",
      " 22  Carbon dioxide [Partial pressure] in Venous blood    19056 non-null   float64\n",
      " 23  Oxygen [Partial pressure] in Venous blood            21382 non-null   float64\n",
      " 24  Albumin [Mass/volume] in Serum or Plasma             12580 non-null   float64\n",
      " 25  Bicarbonate [Moles/volume] in Venous blood           21417 non-null   float64\n",
      " 26  Oxygen [Partial pressure] in Arterial blood          2412 non-null    float64\n",
      " 27  Carbon dioxide [Partial pressure] in Arterial blood  2413 non-null    float64\n",
      " 28  Interleukin 6 [Mass/volume] in Body fluid            60 non-null      float64\n",
      " 29  Magnesium [Moles/volume] in Blood                    11477 non-null   float64\n",
      " 30  Prothrombin time (PT)                                7808 non-null    float64\n",
      " 31  Procalcitonin [Mass/volume] in Serum or Plasma       12580 non-null   float64\n",
      " 32  Lactate [Moles/volume] in Blood                      19827 non-null   float64\n",
      " 33  Creatinine [Mass/volume] in Blood                    25850 non-null   float64\n",
      " 34  Fibrinogen measurement                               7756 non-null    float64\n",
      " 35  Bilirubin measurement                                11401 non-null   float64\n",
      " 36  Partial thromboplastin time                          7757 non-null    float64\n",
      " 37   activated                                           24786 non-null   float64\n",
      " 38  Total white blood count                              24728 non-null   float64\n",
      " 39  Platelet count                                       6360 non-null    float64\n",
      " 40  White blood cell count                               21635 non-null   float64\n",
      " 41  Blood venous pH                                      367 non-null     float64\n",
      " 42  D-dimer level                                        2415 non-null    float64\n",
      " 43  Blood arterial pH                                    27082 non-null   float64\n",
      " 44  Hemoglobin [Moles/volume] in Blood                   24590 non-null   float64\n",
      " 45  drugs                                                66443 non-null   object \n",
      " 46  routes                                               66443 non-null   object \n",
      " 47  visit_occurrence_id                                  66443 non-null   float64\n",
      " 48  procedure                                            195517 non-null  object \n",
      "dtypes: float64(44), int64(1), object(4)\n",
      "memory usage: 124.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data_factors_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_2 = training_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'drugs', 'routes', 'procedure'], dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_factors_2.select_dtypes(object).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_person_ids(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    This function reads person_id column for all rows based on UID and adds a new column with this information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'df': pandas DataFrame\n",
    "        This should be a pandas DataFrame which contains a column 'uid' with universal ids, with strings with format s.t. the person_id starts at the 18th index. e.g.: '2021-07-21 12:00:00623183219'\n",
    "    \"\"\"\n",
    "    # assert that uid column exists\n",
    "    print('Beginning adding new_person_id column based on uids.')\n",
    "    new_person_ids = pd.DataFrame(columns=['new_person_id','uid'])\n",
    "    new_id = df['uid'].copy().apply(lambda x: x[19:])\n",
    "    # new_id.apply(lambda x: int(x))\n",
    "    new_id = new_id.astype(int)\n",
    "    new_person_ids['new_person_id'] = new_id\n",
    "    new_person_ids['uid'] = df['uid'].copy()\n",
    "    # df.drop(columns=['person_id'],inplace=True)\n",
    "    df['new_person_id']=new_person_ids['new_person_id']\n",
    "    print('Finished adding new_person_id column based on uids.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning adding new_person_id column based on uids.\n",
      "Finished adding new_person_id column based on uids.\n"
     ]
    }
   ],
   "source": [
    "new_person_ids(data_factors_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def birthday_ubiquity(df: pd.DataFrame, data_dictionary: dict):\n",
    "    \"\"\"\n",
    "    This function adds age in months to all rows in df based on uid time and birthday. It also adds gender to all rows in df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'df': pandas DataFrame\n",
    "        This is a pandas DataFrame of patient data that contains a 'new_person_id' column from running new_person_ids function.\n",
    "    'data_dictionary': dictionary\n",
    "        This is a dictionary of pandas DataFrames, should have a table of 'person_demographics_episode' data. The 'person_demographics_episode' data will be used to establish birthdays and genders for patients who occur in the DataFrame given.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    'df': pandas DataFrame\n",
    "        returns df with age in months and gender for participants with entry in 'person_demographics_episode' from provided data_dictionary.\n",
    "\n",
    "    Details\n",
    "    -------\n",
    "    * makes temporary dataframe 'unique_demographics_rows' of gender and birthday from 'person_demographics_episode'\n",
    "    * temporarily adds birthday by joining birthday from 'unique_demographics_rows' to df with key as new_person_id\n",
    "    * temporarily creates datetime column derived from uid\n",
    "    * adds age in months column by time between uid date and birthday to df in new column\n",
    "    * deletes birthday column from df\n",
    "    * adds gender column by joining gender from 'unique_demographics_rows' to df with key as new_person_id\n",
    "    \"\"\"\n",
    "    #assert that new_person_id column exists\n",
    "\n",
    "    # factors = pd.merge(left=testing_data['SepsisLabel_test'],right=factors,how='left',on='uid')\n",
    "\n",
    "\n",
    "    demographics_ind_no = np.argmax([table.startswith(\"person_demographics\") for table in data_dictionary.keys()])\n",
    "    demographics_index = list(data_dictionary.keys())[demographics_ind_no]\n",
    "    demographics = data_dictionary[demographics_index]\n",
    "\n",
    "    unique_demographics_rows = pd.DataFrame(columns=['new_person_id','gender','birthday'])\n",
    "    for patient in np.unique(demographics['person_id']):\n",
    "        birthday = list(demographics[demographics['person_id']==patient]['birthday_formatted'])[0]\n",
    "        gender = list(demographics[demographics['person_id']==patient]['gender'])[0]\n",
    "        unique_demographics_rows.loc[len(unique_demographics_rows)] = [patient, gender, birthday]\n",
    "    df = pd.merge(left=df, right=unique_demographics_rows, how='left', on='new_person_id')\n",
    "    datetime_temp = df['uid'].copy().apply(lambda x: x[:19])\n",
    "    datetime_temp = pd.to_datetime(datetime_temp)\n",
    "    birthday_col = df['birthday'].copy()\n",
    "    age = -round((birthday_col-datetime_temp)/np.timedelta64(1,'D')/30)\n",
    "    df['age'] = age\n",
    "    df.drop(columns=['birthday'], inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factors_2 = birthday_ubiquity(df = data_factors_2, data_dictionary = training_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 324753, 1: 6874})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(data_factors_2['SepsisLabel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factors_3 = data_factors_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'drugs', 'routes', 'procedure', 'gender'], dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_factors_3.select_dtypes(object).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearing_cols(df: pd.DataFrame, threshold: int):\n",
    "    \"\"\"\n",
    "    Clears columns that shouldn't be there for modeling and columns that don't meet threshold of number of values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    'df' : pd.DataFrame\n",
    "    'threshold' : int\n",
    "        number of values needed in a column. If column doesn't meet this, it will be dropped from df.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    'df' : pd.DataFrame\n",
    "    \"\"\"\n",
    "    sufficiently_empty_cols = list(df.loc[:,df.count() < threshold].columns)\n",
    "    df.drop(columns=sufficiently_empty_cols, inplace=True)\n",
    "\n",
    "    df.drop(columns=['visit_occurrence_id','uid'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factors_3 = clearing_cols(data_factors_3,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_from_gaussian(column_value, mean: float, std: float):\n",
    "    if np.isnan(column_value) == True:\n",
    "        column_value = np.round(np.random.normal(mean, std, 1)[0], 1)\n",
    "    else:\n",
    "        column_value = column_value\n",
    "    return column_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nans_gaussian(df: pd.DataFrame):\n",
    "    \n",
    "    df['Body temperature'] = df['Body temperature'].apply(fill_from_gaussian, **{'mean': 36.9, 'std': .15})\n",
    "\n",
    "    df.loc[df['age'] <= 12, 'Systolic blood pressure'] = df.loc[df['age'] <= 12, 'Systolic blood pressure'].apply(fill_from_gaussian, **{'mean': 90, 'std': 5})\n",
    "    df.loc[df['age'].between(12, 60,inclusive='right'), 'Systolic blood pressure'] = df.loc[df['age'].between(12, 60,inclusive='right'), 'Systolic blood pressure'].apply(fill_from_gaussian, **{'mean': 105, 'std': 7})\n",
    "    df.loc[df['age'].between(60, 120,inclusive='right'), 'Systolic blood pressure'] = df.loc[df['age'].between(60, 120,inclusive='right'), 'Systolic blood pressure'].apply(fill_from_gaussian, **{'mean': 114, 'std': 7})\n",
    "    df.loc[df['age'] >120, 'Systolic blood pressure'] = df.loc[df['age'] >120, 'Systolic blood pressure'].apply(fill_from_gaussian, **{'mean': 120, 'std': 10})\n",
    "\n",
    "    df.loc[df['age'] <= 12, 'Diastolic blood pressure'] = df.loc[df['age'] <= 12, 'Diastolic blood pressure'].apply(fill_from_gaussian, **{'mean': 49, 'std': 5})\n",
    "    df.loc[df['age'].between(12, 60,inclusive='right'), 'Diastolic blood pressure'] = df.loc[df['age'].between(12, 60,inclusive='right'), 'Diastolic blood pressure'].apply(fill_from_gaussian, **{'mean': 60, 'std': 5})\n",
    "    df.loc[df['age'].between(60, 120,inclusive='right'), 'Diastolic blood pressure'] = df.loc[df['age'].between(60, 120,inclusive='right'), 'Diastolic blood pressure'].apply(fill_from_gaussian, **{'mean': 70, 'std': 5})\n",
    "    df.loc[df['age'] > 120, 'Diastolic blood pressure'] = df.loc[df['age'] > 120, 'Diastolic blood pressure'].apply(fill_from_gaussian, **{'mean': 75, 'std': 7})\n",
    "\n",
    "    df.loc[df['age'] <= 2, 'Hematocrit [Volume Fraction] of Blood'] = df.loc[df['age'] <= 2, 'Hematocrit [Volume Fraction] of Blood'].apply(fill_from_gaussian, **{'mean': 42, 'std': 4})\n",
    "    df.loc[df['age'].between(2, 12,inclusive='right'), 'Hematocrit [Volume Fraction] of Blood'] = df.loc[df['age'].between(2, 12,inclusive='right'), 'Hematocrit [Volume Fraction] of Blood'].apply(fill_from_gaussian, **{'mean': 35, 'std': 4})\n",
    "    df.loc[df['age'].between(12, 60,inclusive='right'), 'Hematocrit [Volume Fraction] of Blood'] = df.loc[df['age'].between(12, 60,inclusive='right'), 'Hematocrit [Volume Fraction] of Blood'].apply(fill_from_gaussian, **{'mean': 37, 'std': 2})\n",
    "    # > 60 Hematocrit should vary for M vs F but not implemented here\n",
    "    df.loc[df['age'] > 60, 'Hematocrit [Volume Fraction] of Blood'] = df.loc[df['age'] > 60, 'Hematocrit [Volume Fraction] of Blood'].apply(fill_from_gaussian, **{'mean': 42, 'std': 2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_nans_gaussian(data_factors_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SepsisLabel', 'Systolic blood pressure', 'Diastolic blood pressure',\n",
       "       'Body temperature', 'Respiratory rate', 'Heart rate',\n",
       "       'Measurement of oxygen saturation at periphery',\n",
       "       'Base excess in Venous blood by calculation',\n",
       "       'Phosphate [Moles/volume] in Serum or Plasma',\n",
       "       'Potassium [Moles/volume] in Blood',\n",
       "       'Bilirubin.total [Moles/volume] in Serum or Plasma',\n",
       "       'Neutrophil Ab [Units/volume] in Serum',\n",
       "       'Hematocrit [Volume Fraction] of Blood',\n",
       "       'Glucose [Moles/volume] in Serum or Plasma',\n",
       "       'Chloride [Moles/volume] in Blood',\n",
       "       'Sodium [Moles/volume] in Serum or Plasma',\n",
       "       'C reactive protein [Mass/volume] in Serum or Plasma',\n",
       "       'Carbon dioxide [Partial pressure] in Venous blood',\n",
       "       'Oxygen [Partial pressure] in Venous blood',\n",
       "       'Albumin [Mass/volume] in Serum or Plasma',\n",
       "       'Bicarbonate [Moles/volume] in Venous blood',\n",
       "       'Magnesium [Moles/volume] in Blood',\n",
       "       'Procalcitonin [Mass/volume] in Serum or Plasma',\n",
       "       'Lactate [Moles/volume] in Blood', 'Creatinine [Mass/volume] in Blood',\n",
       "       'Bilirubin measurement', ' activated', 'Total white blood count',\n",
       "       'White blood cell count', 'Blood arterial pH',\n",
       "       'Hemoglobin [Moles/volume] in Blood', 'new_person_id', 'age',\n",
       "       'encoded_drugs', 'encoded_routes', 'encoded_procedure',\n",
       "       'encoded_gender'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_factors_3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_encoding(df: pd.DataFrame):\n",
    "    categorical_cols = list(df.select_dtypes(object).columns)\n",
    "    le_dictionary = {}\n",
    "    for name in tqdm(categorical_cols):\n",
    "        print(name)\n",
    "        le = LabelEncoder()\n",
    "        le.fit(df.loc[:,f'{name}'])\n",
    "        new_col = le.transform(df.loc[:,f'{name}'])\n",
    "        le_dictionary[name] = le\n",
    "        df.drop(columns=f'{name}', inplace=True)\n",
    "        df[f'encoded_{name}'] = new_col\n",
    "        print(f'finished {name}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:00<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drugs\n",
      "finished drugs\n",
      "routes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:00<00:00,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished routes\n",
      "procedure\n",
      "finished procedure\n",
      "gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_encoding(data_factors_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_join_processing(df: pd.DataFrame, data_dictionary: dict, entry_threshold: int):\n",
    "    \"\"\"\n",
    "    Runs operations on dataframe that occur after the tables of patient data have been joined together into a table of factors. This is pre-split.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'df' : pd.DataFrame\n",
    "        This is a DataFrame of patient data.\n",
    "    'data_dictionary' : dict\n",
    "        This is a dictionary of DataFrames that was presumably used to create the DataFrame df. Data from 'person_demographics_episode' will be used to populate age and gender in df.\n",
    "    'entry_threshold' : int\n",
    "        Columns with count of values less than this threshold will be removed. That is, non-NaN values.\n",
    "    \n",
    "    Details\n",
    "    -------\n",
    "    * runs new_person_ids\n",
    "    * runs birthday_ubiquity\n",
    "    * runs clearing_cols\n",
    "    * runs categorical_encoding\n",
    "    \"\"\"\n",
    "    # add flags\n",
    "    \n",
    "    new_person_ids(df=df)\n",
    "    df = birthday_ubiquity(df = df, data_dictionary = data_dictionary)\n",
    "    df = clearing_cols(df=df, threshold = entry_threshold)\n",
    "    categorical_encoding(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_imputation(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    kNN imputation where neighbors considered are only from the same person via person_id.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'df' : pd.DataFrame\n",
    "        a DataFrame of compiled patient data.\n",
    "    \n",
    "    Returns\n",
    "    'new_df' : pd.Dataframe\n",
    "        a DataFrame with imputed data.\n",
    "    \"\"\"\n",
    "    \n",
    "    imputed_list = []\n",
    "    original_order = list(df.columns)\n",
    "    for person in tqdm(np.unique(df['new_person_id'])):\n",
    "        minimized_df = df[df['new_person_id']==person].copy()\n",
    "\n",
    "        null_col_ind = minimized_df.loc[:,minimized_df.count() == 0].columns\n",
    "        null_cols_list = list(null_col_ind)\n",
    "        impute_col_ind = minimized_df.loc[:,minimized_df.count() != 0].columns\n",
    "        imputable_cols_list = list(impute_col_ind)\n",
    "        null_cols = minimized_df.loc[:,null_cols_list]\n",
    "        imputable_cols = minimized_df.loc[:,imputable_cols_list]\n",
    "\n",
    "        imp = KNNImputer(n_neighbors = 2)\n",
    "        imp.fit(X = imputable_cols)\n",
    "        imputed_rows = list(imp.transform(X = imputable_cols))\n",
    "        \n",
    "        null_list = list(null_cols)\n",
    "        \n",
    "        null_df = pd.DataFrame(null_list, columns=null_col_ind)\n",
    "        imped_df = pd.DataFrame(imputed_rows, columns=impute_col_ind)\n",
    "        merged_df = pd.concat([null_df.reset_index(drop=True),imped_df.reset_index(drop=True)], axis=1)\n",
    "        merged_df = merged_df[original_order]\n",
    "        imputed_list.append(list(merged_df))\n",
    "\n",
    "    new_df = pd.DataFrame(imputed_list,columns=list(df.columns))\n",
    "    new_df.to_csv(f'./processed_data/factors_imputed_by_person_id.csv')\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2640 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'null_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[261], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m imputed_factors \u001b[38;5;241m=\u001b[39m \u001b[43mpersonal_imputation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_factors_3\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[260], line 32\u001b[0m, in \u001b[0;36mpersonal_imputation\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     29\u001b[0m imputed_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(imp\u001b[38;5;241m.\u001b[39mtransform(X \u001b[38;5;241m=\u001b[39m imputable_cols))\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# null_list = list(null_cols)\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m null_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mnull_list\u001b[49m, columns\u001b[38;5;241m=\u001b[39mnull_col_ind)\n\u001b[0;32m     33\u001b[0m imped_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputed_rows, columns\u001b[38;5;241m=\u001b[39mimpute_col_ind)\n\u001b[0;32m     34\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([null_df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),imped_df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'null_list' is not defined"
     ]
    }
   ],
   "source": [
    "imputed_factors = personal_imputation(data_factors_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_factors_3[data_factors_3['new_person_id']==3672059]\n",
    "listed = list(test.loc[:,test.count() != 0].columns)\n",
    "to_imp = test.loc[:,listed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.90</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.15</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.10</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.00</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>4.10</td>\n",
       "      <td>8.50</td>\n",
       "      <td>36.90</td>\n",
       "      <td>111.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>37.8</td>\n",
       "      <td>108.0</td>\n",
       "      <td>...</td>\n",
       "      <td>219.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.30</td>\n",
       "      <td>1.230</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.80</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.15</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.00</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.10</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.90</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.60</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>3.20</td>\n",
       "      <td>9.80</td>\n",
       "      <td>37.80</td>\n",
       "      <td>108.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>38.2</td>\n",
       "      <td>116.0</td>\n",
       "      <td>...</td>\n",
       "      <td>211.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.80</td>\n",
       "      <td>1.240</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.10</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.30</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.10</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.20</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.10</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.20</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.40</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.00</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>37.00</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.00</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>9.15</td>\n",
       "      <td>37.35</td>\n",
       "      <td>109.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>12.55</td>\n",
       "      <td>1.235</td>\n",
       "      <td>3672059.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1    2     3     4      5      6      7     8      9   ...     14  \\\n",
       "0   0.0  35.90 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "1   0.0  36.15 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "2   0.0  36.10 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "3   0.0  36.00 -3.2  4.10  8.50  36.90  111.0  138.0  37.8  108.0  ...  219.0   \n",
       "4   0.0  35.80 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "5   0.0  36.15 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "6   0.0  36.00 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "7   0.0  36.10 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "8   0.0  35.90 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "9   0.0  34.60 -2.8  3.20  9.80  37.80  108.0  139.0  38.2  116.0  ...  211.0   \n",
       "10  0.0  36.10 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "11  0.0  36.30 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "12  0.0  36.10 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "13  0.0  36.20 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "14  0.0  36.10 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "15  0.0  35.20 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "16  0.0  35.40 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "17  0.0  36.00 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "18  0.0  37.00 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "19  0.0  36.00 -3.0  3.65  9.15  37.35  109.5  138.5  38.0  112.0  ...  215.0   \n",
       "\n",
       "      15     16     17         18    19     20    21   22   23  \n",
       "0   7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "1   7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "2   7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "3   7.37  12.30  1.230  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "4   7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "5   7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "6   7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "7   7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "8   7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "9   7.37  12.80  1.240  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "10  7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "11  7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "12  7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "13  7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "14  7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "15  7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "16  7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "17  7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "18  7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "19  7.37  12.55  1.235  3672059.0  85.0  817.0  27.0  0.0  0.0  \n",
       "\n",
       "[20 rows x 24 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = data_factors_3[data_factors_3['new_person_id']==3672059]\n",
    "to_imp = test.loc[:,listed]\n",
    "imp_test = KNNImputer(n_neighbors=2)\n",
    "imp_test.fit(X = to_imp)\n",
    "imped = imp_test.transform(X = to_imp)\n",
    "dfed = pd.DataFrame(imped)\n",
    "\n",
    "reimp_test = KNNImputer(n_neighbors=5)\n",
    "reimp_test.fit(X = to_imp)\n",
    "reimpted = reimp_test.transform(X = to_imp)\n",
    "redfed = pd.DataFrame(reimpted)\n",
    "\n",
    "dfed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoter(df_X: pd.DataFrame, df_y: pd.DataFrame):\n",
    "    sampling_strat = {0: 95, 1: 5}\n",
    "    smote=SMOTE(sampling_strategy=sampling_strat) \n",
    "    X,y=smote.fit_resample(df_X,df_y)\n",
    "    print(\"completed smote\")\n",
    "    person_id_index = np.argmax([column.startswith('new_person_id') for column in data_factors_3.columns])\n",
    "    person_id = df_X.iloc[:,person_id_index].values\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2)\n",
    "    for train_x_index, test_x_index in gss.split(X=X,y=y,groups=person_id):\n",
    "        X_train = X[train_x_index]\n",
    "        X_test = X[test_x_index]\n",
    "        y_train = y[train_x_index]\n",
    "        y_test = y[test_x_index]\n",
    "        person_id_train = person_id[train_x_index]\n",
    "        person_id_test = person_id[test_x_index]\n",
    "        \n",
    "    formatted_data = {}\n",
    "    formatted_data['X_train'] = X_train\n",
    "    formatted_data['X_test'] = X_test\n",
    "    formatted_data['y_train'] = y_train\n",
    "    formatted_data['y_test'] = y_test\n",
    "    formatted_data['person_id_train'] = person_id_train\n",
    "    formatted_data['person_id_test'] = person_id_test\n",
    "\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_modeling(df):\n",
    "\n",
    "    df.reset_index()\n",
    "    person_id_index = np.argmax([column.startswith('new_person_id') for column in data_factors_3.columns])\n",
    "    column_list = []\n",
    "    [column_list.append(i) for i in range(1,person_id_index)]\n",
    "    [column_list.append(i) for i in range(person_id_index+1,len(df.columns.values))]\n",
    "    X = df.iloc[:,column_list].values\n",
    "    y = df.iloc[:,0].values\n",
    "    person_id = df.iloc[:,person_id_index].values\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2)\n",
    "    for train_x_index, test_x_index in gss.split(X=X,y=y,groups=person_id):\n",
    "        X_train = X[train_x_index]\n",
    "        X_test = X[test_x_index]\n",
    "        y_train = y[train_x_index]\n",
    "        y_test = y[test_x_index]\n",
    "        person_id_train = person_id[train_x_index]\n",
    "        person_id_test = person_id[test_x_index]\n",
    "        \n",
    "    formatted_data = {}\n",
    "    formatted_data['X_train'] = X_train\n",
    "    formatted_data['X_test'] = X_test\n",
    "    formatted_data['y_train'] = y_train\n",
    "    formatted_data['y_test'] = y_test\n",
    "    formatted_data['person_id_train'] = person_id_train\n",
    "    formatted_data['person_id_test'] = person_id_test\n",
    "\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = format_for_modeling(data_factors_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_model(data_dictionary: dict):\n",
    "    X_train = data_dictionary['X_train']\n",
    "    X_test = data_dictionary['X_test']\n",
    "    y_train = data_dictionary['y_train']\n",
    "    y_test = data_dictionary['y_test']\n",
    "    person_id_train = data_dictionary['person_id_train']\n",
    "    person_id_test = data_dictionary['person_id_test']\n",
    "\n",
    "    r_forest_model = RandomForestClassifier(class_weight={0:1,1:150})\n",
    "    r_forest_model.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = r_forest_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy score: {accuracy}\")\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    print(f\"F1 score: {f1}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=['not sepsis', 'sepsis'], yticklabels=['not sepsis', 'sepsis'])\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show(block=False)\n",
    "    return r_forest_model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9805445080847013\n",
      "F1 score: 0.2586307782328847\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "forest, preds = forest_model(split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03641424, 0.03909368, 0.03385371, 0.01745375, 0.01503592,\n",
       "       0.02155478, 0.00155354, 0.00072438, 0.00227892, 0.00136183,\n",
       "       0.00358003, 0.00250165, 0.00269669, 0.00215269, 0.00316546,\n",
       "       0.00399205, 0.00125852, 0.00200764, 0.001268  , 0.00206804,\n",
       "       0.00107449, 0.00430882, 0.00396267, 0.00375953, 0.00235042,\n",
       "       0.00193406, 0.00268149, 0.0019166 , 0.00232199, 0.00255392,\n",
       "       0.07574589, 0.38272571, 0.27358726, 0.03640856, 0.01065305])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 19.0 MiB for an array with shape (51979, 48) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 16\u001b[0m\n\u001b[0;32m      7\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m { \n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m300\u001b[39m], \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m], \n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m30\u001b[39m], \n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_leaf_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m9\u001b[39m], \n\u001b[0;32m     12\u001b[0m } \n\u001b[0;32m     14\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(RandomForestClassifier(), \n\u001b[0;32m     15\u001b[0m \t\t\t\t\t\tparam_grid\u001b[38;5;241m=\u001b[39mparam_grid) \n\u001b[1;32m---> 16\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_) \n",
      "File \u001b[1;32mc:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    885\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    887\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[1;32m--> 888\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\metaestimators.py:158\u001b[0m, in \u001b[0;36m_safe_split\u001b[1;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[0;32m    156\u001b[0m         X_subset \u001b[38;5;241m=\u001b[39m X[np\u001b[38;5;241m.\u001b[39mix_(indices, train_indices)]\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     X_subset \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     y_subset \u001b[38;5;241m=\u001b[39m _safe_indexing(y, indices)\n",
      "File \u001b[1;32mc:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\__init__.py:411\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _polars_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[1;32mc:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\__init__.py:208\u001b[0m, in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    207\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m array[:, key]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 19.0 MiB for an array with shape (51979, 48) and data type float64"
     ]
    }
   ],
   "source": [
    "X_train = split_data['X_train']\n",
    "X_test = split_data['X_test']\n",
    "y_train = split_data['y_train']\n",
    "y_test = split_data['y_test']\n",
    "\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [100, 150, 300], \n",
    "    'max_features': ['sqrt', 'log2', None], \n",
    "    'max_depth': [10, 15, 30], \n",
    "    'max_leaf_nodes': [3, 6, 9], \n",
    "} \n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), \n",
    "\t\t\t\t\t\tparam_grid=param_grid) \n",
    "grid_search.fit(X_train, y_train) \n",
    "print(grid_search.best_estimator_) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
